{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd \n",
    "import configparser\n",
    "import time\n",
    "\n",
    "from io import StringIO "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query to AWS Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open(\"config.ini\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = config.get(\"AWS\", \"KEY\")\n",
    "SECRET = config.get(\"AWS\", \"SECRET\")\n",
    "REGION = config.get(\"AWS\", \"REGION\")\n",
    "SCHEMA_NAME = config.get(\"ATHENA\", \"SCHEMA_NAME\")\n",
    "S3_BUCKET_NAME = config.get(\"S3\", \"BUCKET_NAME\")\n",
    "S3_STAGING_DIR = config.get(\"S3\", \"STAGING_DIR\")\n",
    "S3_OUTPUT_DIR = config.get(\"S3\", \"OUTPUT_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to athena service\n",
    "athena_client = boto3.client(\n",
    "    \"athena\",\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET,\n",
    "    region_name=REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_load_query_results(\n",
    "        client: boto3.client,\n",
    "        query_response: dict,\n",
    "        retry: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    while True:\n",
    "        try:\n",
    "            # This function only loads the first 1k rows \n",
    "            # Is this where the query execute?\n",
    "            client.get_query_results(\n",
    "                QueryExecutionId=query_response[\"QueryExecutionId\"]\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if \"not yet finished\" in str(e):\n",
    "                time.sleep(.001)\n",
    "            else: \n",
    "                raise e \n",
    "    # Local tmp data \n",
    "    temp_file_location: str = \"athena_query_results.csv\"\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=KEY,\n",
    "        aws_secret_access_key=SECRET,\n",
    "        region_name=REGION\n",
    "    )\n",
    "    for i in range(retry):\n",
    "        try:\n",
    "            s3_client.download_file(\n",
    "                S3_BUCKET_NAME,\n",
    "                f\"{S3_OUTPUT_DIR}/{query_response['QueryExecutionId']}.csv\",\n",
    "                temp_file_location\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Some thing went wrong for try {i + 1}\")\n",
    "            print(e)\n",
    "            time.sleep(5)\n",
    "            \n",
    "    return pd.read_csv(temp_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query input and config \n",
    "# Limit output to 10,000 for simplicity\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM enigma_jhud LIMIT 10000;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enigma_jhud = download_and_load_query_results(athena_client, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the process for all table\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM nytimes_data_in_usa_us_county LIMIT 10000;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "nytimes_data_in_usa_us_county = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM nytimes_data_in_usa_us_states LIMIT 10000;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "nytimes_data_in_usa_us_states = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM rearc_covid_19_testing_data_states_daily LIMIT 10000;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "rearc_covid_19_testing_data_states_daily = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM rearc_covid_19_testing_data_us_daily LIMIT 10000;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "rearc_covid_19_testing_data_us_daily = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM rearc_usa_hospital_beds LIMIT 10000;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "rearc_usa_hospital_beds = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM rearc_covid_19_testing_data_us_total_latest LIMIT 10000;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "rearc_covid_19_testing_data_us_total_latest = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM static_data_states_abv LIMIT 10000;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "static_data_states_abv = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM static_data_countrypopulation LIMIT 10000;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "static_data_countrypopulation = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM static_data_countrycode LIMIT 10000;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "static_data_countrycode = download_and_load_query_results(athena_client, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle header parsing error \n",
    "static_data_states_abv.columns = [\"State\", \"Abbreviation\", \"partition_0\"]\n",
    "static_data_states_abv = static_data_states_abv[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ETL jobs with python\n",
    "- Implement star schema with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine confirmed case data and testing data\n",
    "factCovid_left = enigma_jhud[[\n",
    "    \"fips\", \n",
    "    \"province_state\",\n",
    "    \"country_region\",\n",
    "    \"confirmed\",\n",
    "    \"deaths\",\n",
    "    \"recovered\",\n",
    "    \"active\",\n",
    "]]\n",
    "\n",
    "factCovid_right = rearc_covid_19_testing_data_states_daily[[\n",
    "    \"fips\", \n",
    "    \"date\",\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"hospitalizedcurrently\",\n",
    "    \"hospitalized\",\n",
    "    \"hospitalizeddischarged\",\n",
    "]]\n",
    "factCovid = pd.merge(\n",
    "    factCovid_left,\n",
    "    factCovid_right,\n",
    "    on=\"fips\",\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1068, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factCovid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimRegion_left = enigma_jhud[[\n",
    "    \"fips\",\n",
    "    \"province_state\",\n",
    "    \"country_region\",\n",
    "    \"latitude\",\n",
    "    \"longitude\"\n",
    "]]\n",
    "\n",
    "dimRegion_right = nytimes_data_in_usa_us_county[[\n",
    "    \"fips\",\n",
    "    \"county\",\n",
    "    \"state\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimRegion = pd.merge(\n",
    "    dimRegion_left,\n",
    "    dimRegion_right,\n",
    "    on=\"fips\",\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['objectid', 'hospital_name', 'hospital_type', 'hq_address',\n",
       "       'hq_address1', 'hq_city', 'hq_state', 'hq_zip_code', 'county_name',\n",
       "       'state_name', 'state_fips', 'cnty_fips', 'fips', 'num_licensed_beds',\n",
       "       'num_staffed_beds', 'num_icu_beds', 'adult_icu_beds', 'pedi_icu_beds',\n",
       "       'bed_utilization', 'avg_ventilator_usage',\n",
       "       'potential_increase_in_bed_capac', 'latitude', 'longtitude',\n",
       "       'partition_0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearc_usa_hospital_beds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimHospital = rearc_usa_hospital_beds[[\n",
    "    \"fips\",\n",
    "    \"latitude\",\n",
    "    \"longtitude\",\n",
    "    \"hq_address\",\n",
    "    \"hospital_name\",\n",
    "    \"hospital_type\",\n",
    "    \"hq_city\",\n",
    "    \"hq_state\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimDate = rearc_covid_19_testing_data_states_daily[[\"fips\", \"date\"]].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimDate[\"date\"] = pd.to_datetime(dimDate[\"date\"], format=r\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimDate[\"year\"] = dimDate.date.dt.year\n",
    "dimDate[\"month\"] = dimDate.date.dt.month\n",
    "dimDate[\"day_of_week\"] = dimDate.date.dt.dayofweek"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### store transformation result to s3 bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_to_s3(data: pd.DataFrame, file_name: str):\n",
    "    \"\"\"Store the dataframe as csv in s3 bucket\"\"\"\n",
    "    # Store the csv file into memory buffer in binary format\n",
    "    csv_buffer = StringIO()\n",
    "    data.to_csv(csv_buffer) # Index = None ?\n",
    "    # Store them into s3 bucket \n",
    "    s3_resource = boto3.resource(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=KEY,\n",
    "        aws_secret_access_key=SECRET,\n",
    "        region_name=REGION\n",
    "    )\n",
    "    s3_resource.Object(S3_BUCKET_NAME, f\"covid/output/{file_name}.csv\")\\\n",
    "        .put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_to_s3(factCovid, \"factCovid\")\n",
    "store_to_s3(dimDate, \"dimDate\")\n",
    "store_to_s3(dimHospital, \"dimHospital\")\n",
    "store_to_s3(dimRegion, \"dimRegion\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Redshift Cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DWH_CLUSTE_TYPE = config.get(\"DWH\", \"DWH_CLUSTE_TYPE\")\n",
    "DWH_NUM_NODES = config.get(\"DWH\", \"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE = config.get(\"DWH\", \"DWH_NODE_TYPE\")\n",
    "DWH_CLUSTER_IDENIFIER = config.get(\"DWH\", \"DWH_CLUSTER_IDENIFIER\")\n",
    "DWH_DB = config.get(\"DWH\", \"DWH_DB\")\n",
    "DWH_DB_USER = config.get(\"DWH\", \"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD = config.get(\"DWH\", \"DWH_DB_PASSWORD\")\n",
    "DWH_PORT = config.get(\"DWH\", \"DWH_PORT\")\n",
    "DWH_IAM_ROLE_NAME = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\n",
    "    'iam',\n",
    "    region_name=\"ap-east-1\",\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET\n",
    ")\n",
    "\n",
    "ec2 = boto3.resource(\n",
    "    'ec2',\n",
    "    region_name=\"ap-east-1\",\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET\n",
    ")\n",
    "\n",
    "redshift = boto3.client(\n",
    "    'redshift',\n",
    "    region_name=\"ap-east-1\",\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier for iam roles \n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)[\"Role\"][\"Arn\"]\n",
    "\n",
    "# Create redshift cluster with code  \n",
    "try:\n",
    "    response = redshift.create_cluster(\n",
    "        ClusterType=DWH_CLUSTE_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        # Credentials & Identifiers\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        # Role for s3 access \n",
    "        IamRoles=[roleArn]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "Not ready yet. 'Endpoint'\n",
      "ec2.SecurityGroup(id='sg-0112e8b0cf043dd33')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        # Loading the redshift cluster info\n",
    "        myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENIFIER)[\"Clusters\"][0]\n",
    "        DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "        DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "        DWH_VPC = myClusterProps['VpcId']\n",
    "        DB_NAME = myClusterProps['DBName']\n",
    "        DB_USER = myClusterProps['MasterUsername']\n",
    "        break\n",
    "    except KeyError as e:\n",
    "        print(\"Not ready yet.\", e)\n",
    "        time.sleep(1)\n",
    "\n",
    "# Configure default security group (inbound role) for redshift cluster \n",
    "try:\n",
    "    vpc = ec2.Vpc(id=DWH_VPC)\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build table and insert data on Redshift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sql table creation command from pandas dataframe \n",
    "factCovidsql = \"\".join(pd.io.sql.get_schema(\n",
    "    factCovid.reset_index(),\n",
    "    \"factCovid\"\n",
    "))\n",
    "dimDatesql = \"\".join(pd.io.sql.get_schema(\n",
    "    dimDate.reset_index(),\n",
    "    \"dimDate\"\n",
    "))\n",
    "dimRegionsql = \"\".join(pd.io.sql.get_schema(\n",
    "    dimRegion.reset_index(),\n",
    "    \"dimRegion\"\n",
    "))\n",
    "dimHospitalsql = \"\".join(pd.io.sql.get_schema(\n",
    "    dimHospital.reset_index(),\n",
    "    \"dimHospital\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dimHospitalsql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting redshift_connector\n",
      "  Downloading redshift_connector-2.0.910-py3-none-any.whl (112 kB)\n",
      "     ------------------------------------ 112.1/112.1 kB 926.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from redshift_connector) (23.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.7.0 in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from redshift_connector) (4.12.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from redshift_connector) (2023.3)\n",
      "Requirement already satisfied: lxml>=4.6.5 in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from redshift_connector) (4.9.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from redshift_connector) (63.2.0)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.9.201 in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from redshift_connector) (1.26.137)\n",
      "Collecting scramp<1.5.0,>=1.2.0\n",
      "  Downloading scramp-1.4.4-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from redshift_connector) (2.30.0)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.12.201 in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from redshift_connector) (1.29.137)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.7.0->redshift_connector) (2.4.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from boto3<2.0.0,>=1.9.201->redshift_connector) (0.6.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from boto3<2.0.0,>=1.9.201->redshift_connector) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from botocore<2.0.0,>=1.12.201->redshift_connector) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from botocore<2.0.0,>=1.12.201->redshift_connector) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->redshift_connector) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->redshift_connector) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->redshift_connector) (3.1.0)\n",
      "Collecting asn1crypto>=1.5.1\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "     -------------------------------------- 105.0/105.0 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\documents\\python-scripts\\data-projects\\.venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.12.201->redshift_connector) (1.16.0)\n",
      "Installing collected packages: asn1crypto, scramp, redshift_connector\n",
      "Successfully installed asn1crypto-1.5.1 redshift_connector-2.0.910 scramp-1.4.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#%pip install redshift_connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redshift_connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = redshift_connector.connect(\n",
    "    host=DWH_ENDPOINT,\n",
    "    database=DB_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DWH_DB_PASSWORD\n",
    ")\n",
    "conn.autocommit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = redshift_connector.Cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<redshift_connector.cursor.Cursor at 0x210126f50f0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table on redshift\n",
    "cursor.execute(factCovidsql)\n",
    "cursor.execute(dimDatesql)\n",
    "cursor.execute(dimHospitalsql)\n",
    "cursor.execute(dimRegionsql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data from s3 \n",
    "try:\n",
    "    cursor.execute(f\"\"\"\n",
    "        copy dimDate from 's3://covid-de-project-thlawab/covid/output/dimDate.csv'\n",
    "        credentials 'aws_iam_role={DWH_ROLE_ARN}'\n",
    "        delimiter ','\n",
    "        region 'ap-east-1'\n",
    "        ignoreheader 1\n",
    "    \"\"\")\n",
    "    cursor.execute(f\"\"\"\n",
    "        copy dimHospital from 's3://covid-de-project-thlawab/covid/output/dimHospital.csv'\n",
    "        credentials 'aws_iam_role={DWH_ROLE_ARN}'\n",
    "        delimiter ','\n",
    "        region 'ap-east-1'\n",
    "        ignoreheader 1;\n",
    "    \"\"\")\n",
    "    cursor.execute(f\"\"\"\n",
    "        copy dimRegion from 's3://covid-de-project-thlawab/covid/output/dimRegion.csv'\n",
    "        credentials 'aws_iam_role={DWH_ROLE_ARN}'\n",
    "        delimiter ','\n",
    "        region 'ap-east-1'\n",
    "        ignoreheader 1;\n",
    "    \"\"\")\n",
    "    cursor.execute(f\"\"\"\n",
    "        copy factCovid from 's3://covid-de-project-thlawab/covid/output/factCovid.csv'\n",
    "        credentials 'aws_iam_role={DWH_ROLE_ARN}'\n",
    "        delimiter ','\n",
    "        region 'ap-east-1'\n",
    "        ignoreheader 1;\n",
    "    \"\"\")\n",
    "except redshift_connector.Error as e:\n",
    "    # Roll back the transaction\n",
    "    conn.rollback()\n",
    "    print(\"Error: Issue copying data to table\")\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
